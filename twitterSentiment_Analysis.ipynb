{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e069752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6da7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a57baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Chirag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Chirag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Chirag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb16d163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tweet_id airline_sentiment    airline airline_sentiment_gold  \\\n",
      "0  567900433542488064          negative  Southwest                    NaN   \n",
      "1  569989168903819264          positive  Southwest                    NaN   \n",
      "2  568089179520954368          positive     United                    NaN   \n",
      "3  568928195581513728          negative  Southwest                    NaN   \n",
      "4  568594180014014464          negative     United                    NaN   \n",
      "\n",
      "            name negativereason_gold  retweet_count  \\\n",
      "0  ColeyGirouard                 NaN              0   \n",
      "1  WalterFaddoul                 NaN              0   \n",
      "2      LocalKyle                 NaN              0   \n",
      "3    amccarthy19                 NaN              0   \n",
      "4        J_Okayy                 NaN              0   \n",
      "\n",
      "                                                text tweet_coord  \\\n",
      "0  @SouthwestAir I am scheduled for the morning, ...         NaN   \n",
      "1  @SouthwestAir seeing your workers time in and ...         NaN   \n",
      "2  @united Flew ORD to Miami and back and  had gr...         NaN   \n",
      "3     @SouthwestAir @dultch97 that's horse radish üò§üê¥         NaN   \n",
      "4  @united so our flight into ORD was delayed bec...         NaN   \n",
      "\n",
      "               tweet_created              tweet_location  \\\n",
      "0  2015-02-17 20:16:29 -0800             Washington D.C.   \n",
      "1  2015-02-23 14:36:22 -0800  Indianapolis, Indiana; USA   \n",
      "2  2015-02-18 08:46:29 -0800                    Illinois   \n",
      "3  2015-02-20 16:20:26 -0800                         NaN   \n",
      "4  2015-02-19 18:13:11 -0800                         NaN   \n",
      "\n",
      "                user_timezone  \n",
      "0      Atlantic Time (Canada)  \n",
      "1  Central Time (US & Canada)  \n",
      "2  Central Time (US & Canada)  \n",
      "3      Atlantic Time (Canada)  \n",
      "4  Eastern Time (US & Canada)  \n",
      "Index(['tweet_id', 'airline_sentiment', 'airline', 'airline_sentiment_gold',\n",
      "       'name', 'negativereason_gold', 'retweet_count', 'text', 'tweet_coord',\n",
      "       'tweet_created', 'tweet_location', 'user_timezone'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Replace with your actual filenames\n",
    "train_df = pd.read_csv(\"training_twitter_x_y_train.csv\")\n",
    "test_df = pd.read_csv(\"test_twitter_x_test.csv\")\n",
    "\n",
    "print(train_df.head())\n",
    "print(train_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df155806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return ADV\n",
    "    else:\n",
    "        return NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1546b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_review(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    words = text.split()\n",
    "    output_words = []\n",
    "    for w in words:\n",
    "        if w.lower() not in stops:\n",
    "            pos = pos_tag([w])\n",
    "            clean_word = lemmatizer.lemmatize(w, pos=get_simple_pos(pos[0][1]))\n",
    "            output_words.append(clean_word.lower())\n",
    "    \n",
    "    return ' '.join(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da2df1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['clean_text'] = train_df['text'].apply(clean_review)\n",
    "test_df['clean_text'] = test_df['text'].apply(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "654ad737",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_df['label'] = le.fit_transform(train_df['airline_sentiment'])\n",
    "y = train_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50e4e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "X = tfidf.fit_transform(train_df['clean_text'])\n",
    "X_test = tfidf.transform(test_df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31cb78b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97570fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7718579234972678\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.95      0.86      1356\n",
      "     neutral       0.64      0.43      0.52       458\n",
      "    positive       0.85      0.55      0.67       382\n",
      "\n",
      "    accuracy                           0.77      2196\n",
      "   macro avg       0.76      0.65      0.68      2196\n",
      "weighted avg       0.77      0.77      0.75      2196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41c37177",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = model.predict(X_test)\n",
    "predictions = le.inverse_transform(final_pred)\n",
    "\n",
    "# Save submission file ‚Äî no header, one column only\n",
    "pd.DataFrame(predictions).to_csv(\"submission.csv\", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
